{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm.topi as topi\n",
    "\n",
    "def batch_norm(c, n, eps=1e-5):\n",
    "    \"\"\"batch normalization\n",
    "\n",
    "    c : channels\n",
    "    N : input width and height\n",
    "    eps : small positive value to prevent divide 0\n",
    "    \"\"\"\n",
    "\n",
    "    X = te.placeholder((c, n, n), name='X')\n",
    "    Mean = te.placeholder((c, 1, 1), name='Mean')\n",
    "    Var = te.placeholder((c, 1, 1), name='Var')\n",
    "    Gamma = te.placeholder((c, 1, 1), name='Gamma')\n",
    "    Beta = te.placeholder((c, 1, 1), name='Beta')\n",
    "    C1 = X - Mean\n",
    "    C2 = topi.sqrt(Var + eps)\n",
    "    Y = C1 / C2 * Gamma + Beta\n",
    "    return X, Mean, Var, Gamma, Beta, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(X_1: handle, Mean_1: handle, Var_1: handle, Gamma_1: handle, Beta_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {X: Buffer(X_2: Pointer(float32), float32, [25088], []),\n",
      "             Mean: Buffer(Mean_2: Pointer(float32), float32, [32], []),\n",
      "             Var: Buffer(Var_2: Pointer(float32), float32, [32], []),\n",
      "             Gamma: Buffer(Gamma_2: Pointer(float32), float32, [32], []),\n",
      "             Beta: Buffer(Beta_2: Pointer(float32), float32, [32], [])}\n",
      "  buffer_map = {X_1: X, Mean_1: Mean, Var_1: Var, Gamma_1: Gamma, Beta_1: Beta}\n",
      "  preflattened_buffer_map = {Var_1: Var_3: Buffer(Var_2, float32, [32, 1, 1], []), Beta_1: Beta_3: Buffer(Beta_2, float32, [32, 1, 1], []), Gamma_1: Gamma_3: Buffer(Gamma_2, float32, [32, 1, 1], []), X_1: X_3: Buffer(X_2, float32, [32, 28, 28], []), Mean_1: Mean_3: Buffer(Mean_2, float32, [32, 1, 1], [])} {\n",
      "  allocate(T_subtract: Pointer(global float32), float32, [25088]), storage_scope = global;\n",
      "  allocate(T_add: Pointer(global float32), float32, [32]), storage_scope = global {\n",
      "    for (ax0: int32, 0, 32) {\n",
      "      for (ax1: int32, 0, 28) {\n",
      "        for (ax2: int32, 0, 28) {\n",
      "          let cse_var_1: int32 = (((ax0*784) + (ax1*28)) + ax2)\n",
      "          T_subtract_1: Buffer(T_subtract, float32, [25088], [])[cse_var_1] = (X[cse_var_1] - Mean[ax0])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0_1: int32, 0, 32) {\n",
      "      T_add_1: Buffer(T_add, float32, [32], [])[ax0_1] = (Var[ax0_1] + 1e-05f32)\n",
      "    }\n",
      "    for (i0: int32, 0, 32) {\n",
      "      T_add_2: Buffer(T_add, float32, [32], [])[i0] = @tir.sqrt(T_add_1[i0], dtype=float32)\n",
      "    }\n",
      "    for (ax0_2: int32, 0, 32) {\n",
      "      for (ax1_1: int32, 0, 28) {\n",
      "        for (ax2_1: int32, 0, 28) {\n",
      "          let cse_var_2: int32 = (((ax0_2*784) + (ax1_1*28)) + ax2_1)\n",
      "          T_subtract_2: Buffer(T_subtract, float32, [25088], [])[cse_var_2] = (T_subtract_1[cse_var_2] / T_add_2[ax0_2])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0_3: int32, 0, 32) {\n",
      "      for (ax1_2: int32, 0, 28) {\n",
      "        for (ax2_2: int32, 0, 28) {\n",
      "          let cse_var_3: int32 = (((ax0_3*784) + (ax1_2*28)) + ax2_2)\n",
      "          T_subtract_3: Buffer(T_subtract, float32, [25088], [])[cse_var_3] = (T_subtract_2[cse_var_3]*Gamma[ax0_3])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (ax0_4: int32, 0, 32) {\n",
      "      for (ax1_3: int32, 0, 28) {\n",
      "        for (ax2_3: int32, 0, 28) {\n",
      "          let cse_var_4: int32 = (((ax0_4*784) + (ax1_3*28)) + ax2_3)\n",
      "          T_subtract_4: Buffer(T_subtract, float32, [25088], [])[cse_var_4] = (T_subtract_3[cse_var_4] + Beta[ax0_4])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 32\n",
    "n = 28\n",
    "X, Mean, Var, Gamma, Beta, Y = batch_norm(c, n)\n",
    "\n",
    "sch = te.create_schedule(Y.op)\n",
    "mod = tvm.build(sch, [X, Mean, Var, Gamma, Beta, Y])\n",
    "\n",
    "print(tvm.lower(sch, [X, Mean, Var, Gamma, Beta], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_data(c, n, constructor=None):\n",
    "    \"\"\"Return the batch norm data, mean, variance, gamma and beta tensors.\n",
    "       Also return the empty tensor for output.\n",
    "\n",
    "    c : channels\n",
    "    n : input width and height\n",
    "    constructor : user-defined tensor constructor\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    data = np.random.normal(size=(c, n, n)).astype('float32')\n",
    "    mean = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    # move the mean of the normal distribution to be 1\n",
    "    var = np.random.normal(loc=1.0, size=(c, 1, 1)).astype('float32')\n",
    "    # make sure all variance numbers are not negative\n",
    "    var = np.absolute(var)\n",
    "    gamma = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    beta = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    out = np.empty((c, n, n), dtype='float32')\n",
    "    if constructor:\n",
    "        data, mean, var, gamma, beta, out = \\\n",
    "        (constructor(x) for x in [data, mean, var, gamma, beta, out])\n",
    "    return data, mean, var, gamma, beta, out\n",
    "\n",
    "data, mean, var, gamma, beta, out = get_bn_data(c, n, tvm.nd.array)\n",
    "mod(data, mean, var, gamma, beta, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "def get_bn_data_mxnet(c, n, ctx='cpu'):\n",
    "    ctx = getattr(mx, ctx)()\n",
    "    data, mean, var, gamma, beta, out = get_bn_data(c, n,\n",
    "                                      lambda x: mx.nd.array(x, ctx=ctx))\n",
    "    data, out = data.expand_dims(axis=0), out.expand_dims(axis=0)\n",
    "    return data, mean, var, gamma, beta, out\n",
    "\n",
    "def batch_norm_mxnet(data, mean, var, gamma, beta, out, eps=1e-5):\n",
    "    # use_global_stats=True to use the input mean and var instead of computing\n",
    "    # the mean and var of the input data.\n",
    "    # fix_gamma=False so that gamma won't be set to 1.\n",
    "    mx.nd.BatchNorm(data, gamma, beta, mean, var, eps,\n",
    "                    use_global_stats=True, fix_gamma=False, out=out)\n",
    "\n",
    "data, mean, var, gamma, beta, out_mx = get_bn_data_mxnet(c, n)\n",
    "batch_norm_mxnet(data, mean, var, gamma, beta, out_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(out_mx[0].asnumpy(), out.asnumpy(), atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af60b091d481e3f69a8e5a5de9b9b5e13d642b6b55b505a9cc6b9b8a84ed9e11"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env_model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
