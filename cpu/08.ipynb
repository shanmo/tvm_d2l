{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te\n",
    "import os\n",
    "import utils \n",
    "\n",
    "os.environ['KMP_AFFINITY']='granularity=fine,noduplicates,compact,1,0'\n",
    "target = 'llvm -mcpu=tigerlake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape (4, 2, 2) \n",
      " [[[ 0.  1.]\n",
      "  [ 2.  3.]]\n",
      "\n",
      " [[ 4.  5.]\n",
      "  [ 6.  7.]]\n",
      "\n",
      " [[ 8.  9.]\n",
      "  [10. 11.]]\n",
      "\n",
      " [[12. 13.]\n",
      "  [14. 15.]]]\n",
      "packed shape (2, 2, 2, 2) \n",
      " [[[[ 0.  4.]\n",
      "   [ 1.  5.]]\n",
      "\n",
      "  [[ 2.  6.]\n",
      "   [ 3.  7.]]]\n",
      "\n",
      "\n",
      " [[[ 8. 12.]\n",
      "   [ 9. 13.]]\n",
      "\n",
      "  [[10. 14.]\n",
      "   [11. 15.]]]]\n"
     ]
    }
   ],
   "source": [
    "c, n, tc = 4, 2, 2  # channel, height/width, and tiling size\n",
    "x = np.arange(c*n*n).reshape((c, n, n)).astype('float32')\n",
    "print('input shape', x.shape, '\\n', x)\n",
    "y = x.reshape(c//tc, n, n, tc).transpose(0, 2, 3, 1)\n",
    "print('packed shape', y.shape, '\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pack(oc, ic, nh, nw, kh, kw, ph, pw, toc, tic):\n",
    "    \"\"\"Pack data and weight for convolution\n",
    "\n",
    "    oc, ic : output and input channels\n",
    "    nh, nw : input width and height\n",
    "    kh, kw : kernel width and height\n",
    "    ph, pw : height and width padding\n",
    "    toc, tic : the tiling sizes of the output and input channels\n",
    "    \"\"\"\n",
    "    X = te.placeholder((ic, nh, nw), name='X')\n",
    "    K = te.placeholder((oc, ic, kh, kw), name='K')\n",
    "    PaddedX = utils.padding(X, ph, pw) if ph * pw != 0 else X\n",
    "    # pack X and K\n",
    "    assert ic % tic == 0 and oc % toc == 0\n",
    "    PackedX = te.compute(\n",
    "        (ic//tic, nh+ph*2, nw+pw*2, tic),\n",
    "        lambda ic_out, x, y, ic_in: PaddedX[ic_out*tic + ic_in, x, y],\n",
    "        name='PackedX')\n",
    "    PackedK = te.compute(\n",
    "        (oc//toc, ic//tic, kh, kw, tic, toc),\n",
    "        lambda oc_out, ic_out, x, y, ic_in, oc_in: K[\n",
    "            oc_out*toc + oc_in, ic_out*tic + ic_in, x, y],\n",
    "        name='PackedK')\n",
    "    return X, K, PaddedX, PackedX, PackedK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _, _, PackedX, _ = conv_pack(c, c, n, n, 1, 1, 0, 0, tc, tc)\n",
    "mod = tvm.build(te.create_schedule(PackedX.op), [X, PackedX])\n",
    "packed_x = tvm.nd.array(np.empty((c//tc, n, n, tc), dtype='float32'))\n",
    "mod(tvm.nd.array(x), packed_x)\n",
    "np.testing.assert_equal(packed_x.asnumpy(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(oc, ic, nh, nw, kh, kw, ph, pw, sh, sw, toc, tic):\n",
    "    \"\"\"2-D conv\n",
    "\n",
    "    oc, ic : output and input channels.\n",
    "    nh, nw : input width and height\n",
    "    kh, kw : kernel width and height\n",
    "    ph, pw : height and width padding\n",
    "    sh, sw : height and width strides\n",
    "    toc, tic : the tiling sizes of output channel and input channel\n",
    "    \"\"\"\n",
    "    X, K, PaddedX, PackedX, PackedK = conv_pack(\n",
    "        oc, ic, nh, nw, kh, kw, ph, pw, toc, tic)\n",
    "    # reduction axes\n",
    "    ric_in = te.reduce_axis((0, tic), name='ric_in')\n",
    "    ric_out = te.reduce_axis((0, ic//tic), name='ric_out')\n",
    "    rkh = te.reduce_axis((0, kh), name='rkh')\n",
    "    rkw = te.reduce_axis((0, kw), name='rkw')\n",
    "    # output height and weights\n",
    "    oh = utils.conv_out_size(nh, kh, ph, sh)\n",
    "    ow = utils.conv_out_size(nw, kw, pw, sw)\n",
    "    # Compuated Y in the packed layout\n",
    "    PackedY = te.compute(\n",
    "        (oc//toc, oh, ow, toc),\n",
    "        lambda oc_out, x, y, oc_in: te.sum(\n",
    "            PackedX[ric_out, x*sh+rkh, y*sw+rkw, ric_in] *\n",
    "            PackedK[oc_out, ric_out, rkh, rkw, ric_in, oc_in],\n",
    "            axis=[ric_out, rkh, rkw, ric_in]), name='Y')\n",
    "    # Unpack the result\n",
    "    Y = te.compute((oc, oh, ow),\n",
    "                    lambda oc, x, y: PackedY[oc//toc, x, y, oc%toc],\n",
    "                    name='Y')\n",
    "    return X, K, Y, PaddedX, PackedX, PackedK, PackedY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc, ic, n, k, p, s, toc, tic = 4, 6, 12, 3, 1, 1, 2, 3\n",
    "X, K, Y, _, _, _, _ = conv(oc, ic, n, n, k, k, p, p, s, s, toc, tic)\n",
    "mod = tvm.build(te.create_schedule(Y.op), [X, K, Y])\n",
    "\n",
    "data, weight, out = utils.get_conv_data(oc, ic, n, k, p, s, tvm.nd.array)\n",
    "mod(data, weight, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, weight, bias, out_mx = utils.get_conv_data_mxnet(oc, ic, n, k, p, s)\n",
    "utils.conv_mxnet(data, weight, bias, out_mx, k, p, s)\n",
    "np.testing.assert_allclose(out_mx[0].asnumpy(), out.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling sizes for output channel, input channel, and width\n",
    "toc, tic, tw = 16, 16, 4\n",
    "\n",
    "def cached_block(oc, ic, n, k, p, s):\n",
    "    X, K, Y, PaddedX, PackedX, PackedK, PackedY = conv(\n",
    "        oc, ic, n, n, k, k, p, p, s, s, toc, tic)\n",
    "    s = te.create_schedule(Y.op)\n",
    "    CachedY = s.cache_write(PackedY, 'local')\n",
    "    oc_out, h, w, oc_in = s[PackedY].op.axis\n",
    "    oc_out_h = s[PackedY].fuse(oc_out, h)\n",
    "    # Parallel on the first two dimensions oc_out and h\n",
    "    s[PackedY].parallel(oc_out_h)\n",
    "    # Optimize the computation of a cached output block\n",
    "    w_out, w_in = s[PackedY].split(w, factor=tw)  # Split the columns\n",
    "    s[CachedY].compute_at(s[PackedY], w_out)\n",
    "    _, _, cw, oc_in = CachedY.op.axis\n",
    "    ric_out, rkh, rkw, ric_in = CachedY.op.reduce_axis\n",
    "    s[CachedY].reorder(ric_out, rkh, rkw, ric_in, cw, oc_in)\n",
    "    s[CachedY].unroll(ric_in)\n",
    "    s[CachedY].unroll(cw)\n",
    "    s[CachedY].vectorize(oc_in)\n",
    "    # Schedule the padding by adding thread-level parallelism\n",
    "    if PaddedX != X:\n",
    "        s[PaddedX].parallel(PaddedX.op.axis[0])\n",
    "    # Optimize the packing of X and K\n",
    "    s[PackedX].parallel(s[PackedX].fuse(*PackedX.op.axis[0:2]))\n",
    "    s[PackedX].unroll(PackedX.op.axis[-1])\n",
    "    s[PackedK].parallel(s[PackedK].fuse(*PackedK.op.axis[0:2]))\n",
    "    s[PackedK].unroll(PackedK.op.axis[-1])\n",
    "    # Optimize the unpacking of Y\n",
    "    s[Y].parallel(s[Y].fuse(*Y.op.axis[0:2]))\n",
    "    s[Y].unroll(Y.op.axis[-1])\n",
    "    return s, (X, K, Y)\n",
    "\n",
    "s, args = cached_block(32, 32, 64, 3, 1, 1)\n",
    "# Uncomment the following line to see the long\n",
    "# psuedo codes because of unrolling.\n",
    "# tvm.lower(s, args, simple_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit \n",
    "\n",
    "def conv_timer_mxnet(c, n, k, ctx):\n",
    "    \"\"\"Benchmark convolution in MXNet\n",
    "    c : input, output channels\n",
    "    n : input width and height\n",
    "    k : kernel width and height\n",
    "    \"\"\"\n",
    "    timer = timeit.Timer(\n",
    "        setup='import utils\\n'\n",
    "        'import mxnet as mx\\n'\n",
    "        'c, n, k, p, s = %d, %d, %d, %d, 1\\n'\n",
    "        'data, weight, bias, out = utils.get_conv_data_mxnet(\\n'\n",
    "        '    c, c, n, k, p, s, \"%s\")'%(c, n, k, (k-1)//2, ctx),\n",
    "        stmt='utils.conv_mxnet(data, weight, bias, out, k, p, s);'\n",
    "        'out.wait_to_read()')\n",
    "    return timer.timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_conv_mxnet(sizes, ctx='cpu'):\n",
    "    \"\"\"Return the GFLOPS of MXNet convolution\"\"\"\n",
    "    return [utils.conv_gflop(c, c, n, k, (k-1)//2, 1) /\n",
    "            utils.bench_workload(conv_timer_mxnet(c, n, k, ctx))\n",
    "            for c, n, k in sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 2**np.arange(4, 9)\n",
    "sizes = [(int(c), 64, 3) for c in channels] # a list of (c, n, k)\n",
    "tvm_gflops = utils.bench_conv_tvm(cached_block, sizes, target)\n",
    "mxnet_gflops = bench_conv_mxnet(sizes)\n",
    "utils.plot_gflops(channels, [mxnet_gflops, tvm_gflops], ['mxnet', 'tvm'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env_model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af60b091d481e3f69a8e5a5de9b9b5e13d642b6b55b505a9cc6b9b8a84ed9e11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
